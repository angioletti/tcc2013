\chapter{Introdução}

%% Histórico

	Ao longo dos últimos quarenta anos, com a difusão de tecnologias digitais, a palavra ``digital'' tem se tornado cada vez mais lugar comum. Ao longo desse período, a tecnologia baseada em máquinas de cálculo automatizado que ocupavam prédios inteiros e eram operadas por poucos (para fins restritos) foi permeando nossa cultura global e ganhando destaque em vários campos do conhecimento e da vida humanas. Com o passar do tempo, as aplicações deixaram de ser restritas, os computadores passaram a ser pessoais e cada vez mais amplamente utilizados (ainda que restritos a mesas e ambientes reservados), tinham aplicações específicas em ambientes de pesquisa e para atividades profissionais. Em paralelo, a indústria de jogos eletrônicos começa a florescer, se fazendo presente nas residências através de seus consoles. Todos aproveitavam a evolução tecnológica apresentada pelo advento dos transistores e microprocessadores.

	Adiante mais alguns anos, e a internet aparece, os microprocessadores estão mais ``micro'' e mais ``processadores'', os consoles de video-game mais elaborados e seus clientes cada vez mais exigentes. Os microprocessadores vão transitando de máquinas estáticas para aplicações móveis: celulares e câmeras digitais (para foto e vídeo) tornam-se quase onipresentes. Mídias digitais tornam-se tão importantes quanto protocolos de comunicação, redes sociais movimentam a opinião pública e servem de plataforma a revoluções [ref. reportagem cairo e facebook]. Muitos cidadãos globais têm uma conta em várias redes sociais e máquinas portáteis em seus bolsos, prontas a fazer um vídeo ou uma foto e postá-los para apreciação popular. Ao que se faz possível, com alguns poucos cliques e nenhum custo com passagem ou hospedagem, termos acesso à realidade atualizada de diversas partes do globo, em todos os idiomas possíveis, a qualquer hora.

	O volume de dados gerado é tão grande que plataformas como o Flickr (da americana Yahoo) já abrigam mais de seis bilhões de imagens digitais [referência ao site], sendo que nos últimos anos a tendência de crescimento de seu banco de imagens foi de um bilhão por ano --- e essa é apenas uma de várias plataformas de hospedagem de publicação de mídias [ref. ao 500px e Instagram]. Outro exemplo, para publicação de vídeos, é o Youtube, onde seus usuários fazem {\em upload} de XX GB de informação por XXX. Se somarmos a esses os números do Vimeo [ref], outra plataforma para vídeos, chegamos a estonteantes XX Gb de vídeo por XX [ref]. Além de serviços de hospedagem de conteúdo disponíveis publicamente, mais recentemente surgiram serviços de {\em streamming} de filmes e séries, como o Netflix[ref], já com mais de XX usuários nos EUA, assistindo XX videos por XXX [ref].

	Esses números representam um desafio para a indústria, que se responsabiliza por receber, armazenar e distribuir esses dados sob demanda, para todo o globo.

	Voltemos um pouco à história de alguns parágrafos acima. Outra entidade que acompanha esse desenvolvimento, e que o antecede em algumas décadas é a televisão, cujo pai aclamado é Philo Farnsworth, entre tantos outros engenheiros e contribuições. [http://en.wikipedia.org/wiki/History_of_television#Electromechanical_television] Desde 1929 existe programação regular para TV sendo projetada no espaço, proveniente de ambos os lados do Atlântico (os Estados Unidos e o Reino Unido começaram a produzir programação regularmente na segunda metade de 1929). [ref]

	Com o nascimento dessa tecnologia, começou-se a discutir a necessidade de avaliar a qualidade da imagem recebida em aparelhos de TV, e artigos como ``{\em Television images: an analysis of their essential qualities}'', publicado por Jesty e Wintch, em 1937, aparecem. Essa discussão vem compor aquela pré-existente da qualidade de imagens fotográficas [ref]. Winch, começa seu artigo de 1952 [ref.] com a afirmação (em tradução livre) "a adição de cores à televisão traz muitos novos problemas a um assunto já complexo". E já em 1940, Peter Goldmark e John Dyer [ref] apresentam quais características são mais importantes ao determinar-se a qualidade de uma imagem (para TV): definição, faixa de contraste, ângulo de visualização (gradation?), brilho, continuidade(frequência?) (para vídeo)(flickering), distorção geométrica, tamanho, cor e ruído. Algumas dessas características viriam a se tornar objetos de estudo da AQI nos anos vindouros, bem como algumas delas seriam bases para o cálculo de métricas de qualidade de imagem (fotografia e vídeo) --- falaremos de algumas delas nesse trabalho.

	Dado que a parte visual de um vídeo é constituída de imagens paradas em sequência, a avaliação da qualidade de vídeo e de imagem andam entrelaçadas desde o início. Tanto o é, que a International Telecommunications Union --- ITU (União Internacional de Telecomunicações, tradução livre) não faz distinção em seus documentos de padronização de qualidade entre vídeo e imagem (ITUT-Tutorial [ref]); e seu grupo especializado para esse fim é chamado {\em Video Quality Experts Group} (VQEG, Grupo de Peritos em Qualidade de Vídeo, tradução livre). Em [ref] (ITUT-Tutorial), encontramos recomendações para avaliação de qualidade de vídeo que podem ser também apliadas a imagens. Nesse trabalho seguiremos essas recomendações.

	Vê-se que, com tanta demanda por imagem e vídeo, e tráfego destes, é necessário que se encontre uma forma de armazená-los eficientemente, acessá-los confiávelmente e garantir que o usuário final terá a qualidade esperada, ainda que compressões e descarte de informações sejam necessários. Enquanto alguém preocupado com a compressão de uma imagem se perguntaria "qual a menor quantidade de informação necessária para que eu mantenha a completude da mensagem (no nosso caso, imagem)?", um pesquisador de AQI se pergunta: "como medir a qualidade de uma imagem?"

	Essa é uma pergunta razoávelmente complexa, já que envolve conceitos abstratos e subjetivos. Uma boa imagem para uma aplicação não o é, necessáriamente para outra. Um exemplo simples é a aquisição de vídeos de segurança em comparação com a aquisição de vídeo para entretenimento. No primeiro caso, a qualidade mínima e suficiente é aquela que garante a identificação de um possível infrator; na segunda as exigências são mais altas, ninguém sentiria prazer em assistir toda a saga de Star Wars em preto e branco e com todo o ruído e baixa resolução que são aceitáveis para câmeras de segurança. É claro, não se pode deixar de lado as considerações sobre custo-benefício: no primeiro caso a resolução tem que ser mínima e suficiente para a identificação de um eventual infrator, mas também tem que ocupar pouco espaço em disco, já que câmeras, em sua maioria, funcionam continuamente. Quanto a Star Wars, o tamanho é fixo, e a garantia da qualidade do produto final significa aumento de lucros em bilheterias pelo mundo afora.

	Tradicionalmente, existem duas formas de se construir algoritmos de AQI que possam determinar automaticamente a adequação de uma imagem para consumo humano [ref Seven Challenges..]: algoritmos baseados no sistema visual humano (SVH) e algoritmos baseados em sessões de avaliação.

	Algoritmos baseados em SVH levam em consideração a física de todo um sistema, bem como a parte psicológica da percepção visual. Esse trabalho não tratará desse tipo de algoritmo e o leitor é direcionado aos trabalhos de [ref, ref] para maiores informações. A segunda abordagem, que será alvo de estudo nesse trabalho, não aborda a psicofísica e se concentra em características da imagem que sejam relevantes. Para obter esses dados, sessões de avaliação são organizadas, onde pessoas são questionadas a respeito da qualidade de um conjunto de imagens. Após coletados os dados, o pesquisador tenta produzir um modelo que tenha como saída uma nota aproximada daquela dada pelos entrevistados. Obviamente, quanto menor o erro entre o sistema modelado e a avalição subjetiva dos indivíduos entrevistados, tanto melhor é o modelo.

	Aqui trabalharemos com duas bases de imagens distintas, proveniente de grupos de pesquisa independentes, que coletaram avaliação para as imagens constantes em suas bases de forma muito similar. Trataremos dessa similaridade, e eventual diferença no cap /ref{}. Ambos os grupos de pesquisa, seguem o que é de praxe na área, e tratam seus procedimentos de avaliação estatisticamente com muito critério.

	O questionamento que propomos é justamente sobre as ferramentas estatísticas utilizadas no tratamento desses dados. Entendemos que, dada a natureza dos experimentos de avaliação e a forma como as notas são atribuídas, ferramentas de estatística em classes seriam mais adequadas para a análise dos resultados, e consequente pondereção de qualidade. Atualmente, as ferramentas estatísticas utilizadas nesse contexto são as mesmas utilizadas para tratar dados intrinsecamente não-categóricos.

	

	Esse documento está estruturado em X partes, resumidamente descritas abaixo:

\begin{description}
\item{\bf Introdução: } breve histórico da área de Avaliação de Qualidade de Imagem, breve descrição do objetivo do trabalho e descrição suscinta das ferramentas utilizadas
\item{\bf Antecedentes: } detalhamos melhor nossa proposta de trabalho, situamos o leitor quanto às abordagens estatísticas utilizadas na área e nossas ferramentas de comparação.
\item{\bf Procedimentos Experimentais: } apresentamos as bases de imagem que serão utilizadas, os métodos utilizados e resultados preliminares de comparação com a literatura existente.
\item{\bf Análise Categórica: } aprofundamos a explicação das ferramentas utilizadas, aplicamos essas ferramentas aos dados apresentados nos capítulos anteriores e efetuamos comparações de sua validade.
\item{Conclusão: } considerações finais sobre o trabalho e possíveis caminhos a serem tomados a partir das conclusões apresentadas.
\item{Referências: } lista de documentos que serviram de base para a produção desta obra.

%% A relevância da área

